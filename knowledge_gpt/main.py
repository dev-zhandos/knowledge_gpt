import streamlit as st

from knowledge_gpt.components.sidebar import sidebar

from knowledge_gpt.ui import (
    wrap_doc_in_html,
    is_query_valid,
    is_file_valid,
    is_open_ai_key_valid,
    display_file_read_error,
)

from knowledge_gpt.core.caching import bootstrap_caching

from knowledge_gpt.core.parsing import read_file
from knowledge_gpt.core.chunking import chunk_file
from knowledge_gpt.core.embedding import embed_files
from knowledge_gpt.core.qa import query_folder
from knowledge_gpt.core.utils import get_llm


EMBEDDING = "openai"
VECTOR_STORE = "faiss"
MODEL_LIST = ["gpt-3.5-turbo", "gpt-4o"]

# Uncomment to enable debug mode
# MODEL_LIST.insert(0, "debug")

st.set_page_config(page_title="PedagogGPT", page_icon="üìñ", layout="wide")
st.header("üìñPedagogGPT")

# Enable caching for expensive functions
bootstrap_caching()

sidebar()

openai_api_key = st.session_state.get("OPENAI_API_KEY")


if not openai_api_key:
    st.warning(
        "OpenAI API –∫—ñ–ª—Ç—ñ–Ω –±“Ø–π—ñ—Ä —Ç–∞“õ—Ç–∞—Å—ã–Ω–∞ –µ–Ω–≥—ñ–∑—ñ“£—ñ–∑. –ö—ñ–ª—Ç—Ç—ñ OpenAI –≤–µ–±-—Å–∞–π—Ç—ã–Ω–∞–Ω –∞–ª–∞ –∞–ª–∞—Å—ã–∑."
        " https://platform.openai.com/account/api-keys."
    )


uploaded_file = st.file_uploader(
    "PDF, DOCX, –Ω–µ–º–µ—Å–µ TXT —Ñ–∞–π–ª–¥—ã –∂“Ø–∫—Ç–µ“£—ñ–∑.",
    type=["pdf", "docx", "txt"],
    help="–°–∫–∞–Ω–µ—Ä–ª–µ–Ω–≥–µ–Ω “õ“±–∂–∞—Ç—Ç–∞—Ä “õ–æ–ª–¥–∞—É—ã –∂–∞“õ—ã–Ω –∞—Ä–∞–¥–∞ –±–æ–ª–∞–¥—ã!",
)

model: str = st.selectbox("Model", options=MODEL_LIST)  # type: ignore

with st.expander("“ö–æ—Å—ã–º—à–∞ –ü–∞—Ä–∞–º–µ—Ç—Ä–ª–µ—Ä"):
    return_all_chunks = st.checkbox("–ë–∞—Ä–ª—ã“õ –≤–µ–∫—Ç–æ—Ä–ª—ã“õ —ñ–∑–¥–µ—É–¥–µ–Ω –∞–ª—ã–Ω“ì–∞–Ω –∫–µ—Å–µ–∫—Ç–µ—Ä–¥—ñ –∫”©—Ä—Å–µ—Ç—É")
    show_full_doc = st.checkbox("“ö“±–∂–∞—Ç—Ç—ã“£ —Ç–∞–ª–¥–∞–Ω“ì–∞–Ω –º–∞–∑–º“±–Ω—ã–Ω –∫”©—Ä—Å–µ—Ç—É")


if not uploaded_file:
    st.stop()

try:
    file = read_file(uploaded_file)
except Exception as e:
    display_file_read_error(e, file_name=uploaded_file.name)

chunked_file = chunk_file(file, chunk_size=300, chunk_overlap=0)

if not is_file_valid(file):
    st.stop()


if not is_open_ai_key_valid(openai_api_key, model):
    st.stop()


with st.spinner("“ö“±–∂–∞—Ç—Ç—ã –∏–Ω–¥–µ–∫—Å—Ç–µ—É... –ë—ñ—Ä–∞–∑ —É–∞“õ—ã—Ç –∞–ª—É—ã –º“Ø–º–∫—ñ–Ω.‚è≥"):
    folder_index = embed_files(
        files=[chunked_file],
        embedding=EMBEDDING if model != "debug" else "debug",
        vector_store=VECTOR_STORE if model != "debug" else "debug",
        openai_api_key=openai_api_key,
    )

with st.form(key="qa_form"):
    query = st.text_area("“ö“±–∂–∞—Ç —Ç—É—Ä–∞–ª—ã —Å“±—Ä–∞“õ “õ–æ–π—ã“£—ã–∑.")
    submit = st.form_submit_button("Submit")


if show_full_doc:
    with st.expander("Document"):
        # Hack to get around st.markdown rendering LaTeX
        st.markdown(f"<p>{wrap_doc_in_html(file.docs)}</p>", unsafe_allow_html=True)


if submit:
    if not is_query_valid(query):
        st.stop()

    # Output Columns
    answer_col, sources_col = st.columns(2)

    llm = get_llm(model=model, openai_api_key=openai_api_key, temperature=0)
    result = query_folder(
        folder_index=folder_index,
        query=query,
        return_all=return_all_chunks,
        llm=llm,
    )

    with answer_col:
        st.markdown("#### –ñ–∞—É–∞–ø")
        st.markdown(result.answer)

    with sources_col:
        st.markdown("#### –î–µ—Ä–µ–∫–∫”©–∑–¥–µ—Ä")
        for source in result.sources:
            st.markdown(source.page_content)
            st.markdown(source.metadata["source"])
            st.markdown("---")
